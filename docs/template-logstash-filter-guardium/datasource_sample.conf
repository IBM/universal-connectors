# Sample Logstash Configuration for [DATASOURCE_NAME] Guardium Filter Plugin
#
# INSTRUCTIONS:
# 1. Replace [DATASOURCE_NAME] with your data source name
# 2. Replace DATASOURCE_PLACEHOLDER with your data source identifier
# 3. Update the input section based on how you receive logs
# 4. Customize filter conditions as needed
# 5. Save this file as: datasource_name.conf (e.g., mongodb.conf, postgresql.conf)

input {
	# ========== OPTION 1: File Input ==========
	# Use this if reading logs from files
	# file {
	#     path => "/path/to/your/audit/logs/*.log"
	#     start_position => "beginning"
	#     sincedb_path => "/dev/null"
	#     type => "DATASOURCE_PLACEHOLDER"
	#     codec => "json"  # Use "plain" for text logs
	# }

	# ========== OPTION 2: Kafka Input ==========
	# Use this if receiving logs from Kafka
	# kafka {
	#     bootstrap_servers => "localhost:9092"
	#     topics => ["datasource-audit-logs"]
	#     type => "DATASOURCE_PLACEHOLDER"
	#     codec => "json"
	# }

	# ========== OPTION 3: HTTP Input ==========
	# Use this if receiving logs via HTTP
	# http {
	#     port => 8080
	#     type => "DATASOURCE_PLACEHOLDER"
	#     codec => "json"
	# }

	# ========== OPTION 4: Beats Input (Filebeat) ==========
	# Use this if using Filebeat to ship logs
	# beats {
	#     port => 5044
	#     type => "DATASOURCE_PLACEHOLDER"
	# }

	# ========== OPTION 5: Cloud-specific inputs ==========
	# For AWS CloudWatch:
	# cloudwatch_logs {
	#     log_group => "/aws/datasource/audit"
	#     region => "us-east-1"
	#     type => "DATASOURCE_PLACEHOLDER"
	# }

	# For Azure Event Hub:
	# azure_event_hubs {
	#     event_hub_connections => ["Endpoint=sb://..."]
	#     threads => 2
	#     decorate_events => true
	#     consumer_group => "$Default"
	#     storage_connection => "DefaultEndpointsProtocol=https;..."
	#     type => "DATASOURCE_PLACEHOLDER"
	# }

	# For GCP Pub/Sub:
	# google_pubsub {
	#     project_id => "your-project-id"
	#     topic => "datasource-audit-logs"
	#     subscription => "logstash-subscription"
	#     json_key_file => "/path/to/key.json"
	#     type => "DATASOURCE_PLACEHOLDER"
	# }

	# TODO: Uncomment and configure the appropriate input for your use case
	# Example: Reading from a file
	file {
		path => "/var/log/datasource/audit.log"
		start_position => "beginning"
		type => "DATASOURCE_PLACEHOLDER"
		codec => "json"
	}
}

filter {
	# Only process events of this type
	if [type] == "DATASOURCE_PLACEHOLDER" {
		
		# ========== Optional: Pre-processing ==========
		# Add any pre-processing steps here
		
		# Example: Parse timestamp if not already in ISO format
		# date {
		#     match => ["timestamp", "yyyy-MM-dd HH:mm:ss", "ISO8601"]
		#     target => "@timestamp"
		# }

		# Example: Add custom fields
		# mutate {
		#     add_field => {
		#         "environment" => "production"
		#         "data_source" => "[DATASOURCE_NAME]"
		#     }
		# }

		# Example: Remove unnecessary fields
		# mutate {
		#     remove_field => ["@version", "host"]
		# }

		# ========== Main Filter Plugin ==========
		# This is where the Guardium filter plugin is applied
		DATASOURCE_PLACEHOLDER_guardium_filter {
			source => "message"
		}

		# ========== Optional: Post-processing ==========
		# Add any post-processing steps here

		# Example: Add tags for monitoring
		# if "_guardiumrecord" in [tags] {
		#     mutate {
		#         add_tag => ["processed_successfully"]
		#     }
		# }
	}
}

output {
	# ========== Guardium Output ==========
	# Send processed events to Guardium
	if [GuardRecord] {
		# This output is automatically configured by Guardium Universal Connector
		# No additional configuration needed here
	}

	# ========== Optional: Debug Output ==========
	# Uncomment for debugging - writes to stdout
	# stdout {
	#     codec => rubydebug
	# }

	# ========== Optional: File Output for Debugging ==========
	# Uncomment to write processed events to a file
	# file {
	#     path => "/var/log/logstash/datasource-guardium-output.log"
	#     codec => json_lines
	# }

	# ========== Optional: Error Handling ==========
	# Send failed events to a separate output
	# if "_DATASOURCE_PLACEHOLDER_json_parse_error" in [tags] {
	#     file {
	#         path => "/var/log/logstash/datasource-errors.log"
	#         codec => json_lines
	#     }
	# }
}

# ========== Configuration Notes ==========
#
# 1. The 'type' field must match in input and filter sections
# 2. The 'type' field should be unique for each connector
# 3. Adjust codec based on your log format (json, plain, multiline, etc.)
# 4. For production, consider:
#    - Setting appropriate buffer sizes
#    - Configuring persistent queues
#    - Setting up monitoring and alerting
#    - Implementing proper error handling
#
# 5. Performance tuning:
#    - Adjust worker threads: pipeline.workers
#    - Adjust batch size: pipeline.batch.size
#    - Configure memory: -Xms and -Xmx in jvm.options
#
# 6. Security considerations:
#    - Use SSL/TLS for network inputs
#    - Secure credential storage
#    - Implement proper access controls
#    - Enable audit logging for Logstash itself

