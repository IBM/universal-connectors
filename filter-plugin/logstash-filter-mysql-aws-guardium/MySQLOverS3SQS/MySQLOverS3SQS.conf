input {
  s3_sqs {
    queue_url => "<queue_url>" # For i.e https://sqs.<region>.amazonaws.com/<account_id>/<queue_name>
    region => "<region>"
    access_key_id => "<access_key_id>"
    secret_access_key => "<secret_access_key>"
    role_arn => "<role_arn>" # Leave empty if not using role-based access
    max_messages => <max_messages>
    wait_time => <wait_time> # Must be >= 0 and <= 20,
    polling_frequency => <polling_frequency>
    type => "S3SQS_MYSQL"
  }
}

filter {
if [type] == "S3SQS_MYSQL" {

  # Step 1: Parse the JSON message from S3 event into [cloudwatch]
  json {
    source => "message"
    target => "cloudwatch"
  }

  # Step 2: Split logEvents array into separate events
  split {
    field => "[cloudwatch][logEvents]"
  }

  # Step 3: Extract each log message and promote to top-level [message]
  mutate {
    rename => { "[cloudwatch][logEvents][message]" => "message" }
    add_field => {
      "logGroup"  => "%{[cloudwatch][logGroup]}"
      "logStream" => "%{[cloudwatch][logStream]}"
    }
  }

  # Step 4: Drop known noise events based on [message] content
  if [message] =~ /(session\.transaction_read_only|information_schema\.TABLES|GLOBAL\.read_only|information_schema\.rds_events_threads_waits_current|SELECT\s+1|oscar_local_only_replica_host_status|replica_host_status)/ {
    drop { }
  }

  # Step 5: Parse message into fields using grok (use exact field names the plugin expects)
  grok {
    match => {
      "message" => [
        # Format 1: QUERY with database and query
        "%{NUMBER:timestamp_micro},%{DATA:db_instance},%{DATA:user},%{DATA:client_ip},%{NUMBER:thread_id},%{NUMBER:query_id},%{WORD:command},%{DATA:database},'%{GREEDYDATA:query}',%{NUMBER:status_code}",
        
        # Format 2: QUERY without database (empty field - double comma)
        "%{NUMBER:timestamp_micro},%{DATA:db_instance},%{DATA:user},%{DATA:client_ip},%{NUMBER:thread_id},%{NUMBER:query_id},%{WORD:command},,'%{GREEDYDATA:query}',%{NUMBER:status_code}",
        
        # Format 3: READ/WRITE operations (ends with comma, no query)
        "%{NUMBER:timestamp_micro},%{DATA:db_instance},%{DATA:user},%{DATA:client_ip},%{NUMBER:thread_id},%{NUMBER:query_id},%{WORD:command},%{DATA:database},%{DATA:table_name},",
        
        # Format 4: CONNECT/DISCONNECT (multiple empty fields)
        "%{NUMBER:timestamp_micro},%{DATA:db_instance},%{DATA:user},%{DATA:client_ip},%{NUMBER:thread_id},%{NUMBER:query_id},%{WORD:command},,,,%{NUMBER:status_code}"
      ]
    }
    remove_field => ["message"]
    tag_on_failure => ["_grokparsefailure_custom"]
  }

  # Step 5.1: Convert microsecond timestamp to datetime
  date {
    match => ["timestamp_micro", "UNIX_MS"]
    target => "timestamp"
  }

  # Step 5.2: Drop events that failed grok parsing
  if "_grokparsefailure_custom" in [tags] {
    drop { }
  }

   # Step 5.3: Drop events from rdsadmin user (case-insensitive)
  if [user] =~ /(?i)rdsadmin/ {
    drop { }
  }

  # Step 6: Set defaults for missing fields (CRITICAL: prevent NullPointerException)
  if ![user] or [user] == "" {
    mutate { replace => { "user" => "unknown" } }
  }

  if ![command] or [command] == "" {
    mutate { replace => { "command" => "UNKNOWN" } }
  }

  if ![database] or [database] == "" {
    mutate { replace => { "database" => "unknown" } }
  }

  # CRITICAL: Set default status_code to prevent NullPointerException in Parser.java:83
  if ![status_code] {
    mutate { add_field => { "status_code" => "0" } }
  }

  # Step 7: Escape quotes in query field to prevent JSON malformation
  if [query] {
    ruby {
      code => '
        query = event.get("query")
        if query
          query = query.gsub("\\", "\\\\\\\\").gsub("\"", "\\\\\"")
          event.set("query", query)
        end
      '
    }
  }

  # Step 8: Run Guardium plugin
  mysql_guardium_plugin_filter{}

  # Optional: Keep only the GuardRecord field if desired
  prune { 
    whitelist_names => [ "GuardRecord" ] 
  }
}
